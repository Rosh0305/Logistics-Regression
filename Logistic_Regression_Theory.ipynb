{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "1o71J6biqK6O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Theoretical"
      ],
      "metadata": {
        "id": "JBLEonmQrBtJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q1. What is Logistic Regression, and how does it differ from Linear Regression?"
      ],
      "metadata": {
        "id": "M4CqSpDErCi1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans. Linear Regression is used for predicting continuous values (e.g., prices, temperatures) with a linear relationship between input features and the output.\n",
        "\n",
        "Logistic Regression is used for classification tasks (e.g., spam vs. not spam) and outputs probabilities between 0 and 1, using the logistic (sigmoid) function.\n",
        "\n",
        "Key difference: Linear Regression predicts continuous values, while Logistic Regression predicts probabilities for categorical outcomes."
      ],
      "metadata": {
        "id": "s-Y-9lQPre0n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q2. What is the mathematical equation of Logistic Regression?"
      ],
      "metadata": {
        "id": "JWmvN9hQrgpG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans. The mathematical equation for Logistic Regression is:\n",
        "\n",
        "ğ‘ƒ ( ğ‘Œ = 1 âˆ£ ğ‘‹ ) = 1 1 + ğ‘’ âˆ’ ( ğ›½ 0 + ğ›½ 1 ğ‘‹ 1 + ğ›½ 2 ğ‘‹ 2 + . . . + ğ›½ ğ‘› ğ‘‹ ğ‘› ) P(Y=1âˆ£X)= 1+e âˆ’(Î² 0â€‹+Î² 1â€‹X 1â€‹+Î² 2â€‹X 2â€‹+...+Î² nâ€‹X nâ€‹)\n",
        "\n",
        "1â€‹\n",
        "\n",
        "Where:\n",
        "\n",
        "ğ‘ƒ ( ğ‘Œ = 1 âˆ£ ğ‘‹ ) P(Y=1âˆ£X) is the probability of the outcome being 1 (positive class), ğ›½ 0 , ğ›½ 1 , . . . ğ›½ ğ‘› Î² 0â€‹,Î² 1â€‹,...Î² nâ€‹are the model coefficients, ğ‘‹ 1 , ğ‘‹ 2 , . . . ğ‘‹ ğ‘› X 1â€‹,X 2â€‹,...X nâ€‹are the input features, ğ‘’ e is the base of the natural logarithm (Euler's number). This equation maps the linear combination of inputs to a probability between 0 and 1 using the sigmoid function."
      ],
      "metadata": {
        "id": "xpQwUpyXrmqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q3. Why do we use the Sigmoid function in Logistic Regression?"
      ],
      "metadata": {
        "id": "kw_zQRp2rnrn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans. The sigmoid function is used in Logistic Regression because it maps any real-valued input to a probability between 0 and 1. This makes it ideal for classification tasks, as it helps in predicting the likelihood of a sample belonging to a particular class (e.g., class 1 or class 0). The functionâ€™s \"S\" shaped curve ensures smooth transitions between the two classes and outputs values suitable for decision-making (like thresholding at 0.5)."
      ],
      "metadata": {
        "id": "pcsD47zVrsO9"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "irno7_4xrfJf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q4. What is the cost function of Logistic Regression?"
      ],
      "metadata": {
        "id": "UyWiJUQKrtyu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans. The cost function of Logistic Regression is the Log Loss (or Binary Cross-Entropy) and is given by:\n",
        "\n",
        "ğ½ ( ğ›½ ) = âˆ’ 1 ğ‘š âˆ‘ ğ‘– = 1 ğ‘š [ ğ‘¦ ( ğ‘– ) log â¡ ( â„ ğ›½ ( ğ‘¥ ( ğ‘– ) ) ) + ( 1 âˆ’ ğ‘¦ ( ğ‘– ) ) log â¡ ( 1 âˆ’ â„ ğ›½ ( ğ‘¥ ( ğ‘– ) ) ) ] J(Î²)=âˆ’ m 1â€‹\n",
        "\n",
        "i=1 âˆ‘ mâ€‹[y (i) log(h Î²â€‹(x (i) ))+(1âˆ’y (i) )log(1âˆ’h Î²â€‹(x (i) ))]\n",
        "\n",
        "Where:\n",
        "\n",
        "ğ‘š m is the number of training examples, ğ‘¦ ( ğ‘– ) y (i) is the actual label (0 or 1) for the ğ‘– i-th example, â„ ğ›½ ( ğ‘¥ ( ğ‘– ) ) h Î²â€‹(x (i) ) is the predicted probability for the ğ‘– i-th example using the sigmoid function, log â¡ log is the natural logarithm. This function measures how well the model's predictions match the actual labels, with the goal of minimizing the cost to improve accuracy."
      ],
      "metadata": {
        "id": "0bFwyMSnrzTq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q5. What is Regularization in Logistic Regression? Why is it needed?"
      ],
      "metadata": {
        "id": "_KfZUIVCr4Uv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans. Regularization in Logistic Regression is a technique used to prevent overfitting by adding a penalty term to the cost function. It discourages overly complex models that might fit noise in the training data.\n",
        "\n",
        "There are two common types of regularization:\n",
        "\n",
        "L2 Regularization (Ridge): Adds the sum of squared coefficients ( ğœ† âˆ‘ ğ‘— = 1 ğ‘› ğ›½ ğ‘— 2 Î»âˆ‘ j=1 nâ€‹Î² j 2â€‹) to the cost function.\n",
        "\n",
        "L1 Regularization (Lasso): Adds the sum of absolute values of the coefficients ( ğœ† âˆ‘ ğ‘— = 1 ğ‘› âˆ£ ğ›½ ğ‘— âˆ£ Î»âˆ‘ j=1 nâ€‹âˆ£Î² jâ€‹âˆ£) to the cost function.\n",
        "\n",
        "Why is it needed?\n",
        "\n",
        "Prevents overfitting by controlling the model complexity.\n",
        "Helps in improving generalization to new, unseen data."
      ],
      "metadata": {
        "id": "YP2TBb6pr-Aa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q6. Explain the difference between Lasso, Ridge, and Elastic Net regression"
      ],
      "metadata": {
        "id": "8WnEKYhOsATQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans. 1. Ridge Regression (L2): Adds a penalty based on the squared coefficients, shrinking them but not setting them to zero. Useful when all features are important.\n",
        "\n",
        "Lasso Regression (L1): Adds a penalty based on the absolute values of coefficients, shrinking some coefficients to zero, effectively performing feature selection. Useful for identifying important features.\n",
        "\n",
        "Elastic Net: Combines both Lasso and Ridge penalties, offering a balance between feature selection and handling correlated features. Useful when you have many irrelevant or correlated predictors."
      ],
      "metadata": {
        "id": "s6-bjbHzsDaz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q7. When should we use Elastic Net instead of Lasso or Ridge?"
      ],
      "metadata": {
        "id": "lU5mb55AsFlw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans. Use Elastic Net when:\n",
        "\n",
        "You have many correlated features (Ridge can handle correlation, Lasso struggles).\n",
        "\n",
        "You want a balance between feature selection (Lasso) and regularization (Ridge).\n",
        "\n",
        "Youâ€™re unsure whether Lasso or Ridge is more appropriate for your data, as Elastic Net combines both approaches."
      ],
      "metadata": {
        "id": "UyyxE1VwsVf0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q8. What is the impact of the regularization parameter (Î») in Logistic Regression ?"
      ],
      "metadata": {
        "id": "QJeiLBmqsYm-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans. The regularization parameter Î» controls the strength of the regularization in Logistic Regression:\n",
        "\n",
        "Small Î»: Less regularization, allowing the model to fit the training data more closely, which could lead to overfitting.\n",
        "\n",
        "Large Î»: Stronger regularization, which shrinks the coefficients more, potentially underfitting the model by making it too simple.\n",
        "\n",
        "Choosing the right Î» helps balance model complexity and generalization, preventing overfitting while still capturing the essential patterns in the data."
      ],
      "metadata": {
        "id": "RxsmEqiisqOC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q9. What are the key assumptions of Logistic Regression?"
      ],
      "metadata": {
        "id": "_jLaBUDVsuz0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans. The key assumptions of Logistic Regression are:\n",
        "\n",
        "Linearity of Log-Odds: The relationship between the independent variables and the log-odds of the outcome is linear.\n",
        "\n",
        "Independence of Errors: The observations are independent of each other.\n",
        "\n",
        "No Multicollinearity: The independent variables should not be highly correlated with each other.\n",
        "\n",
        "Binary or Multinomial Outcome: The dependent variable should be categorical, typically binary (0 or 1), but it can also handle multi-class problems.\n",
        "\n",
        "These assumptions help the model produce reliable and interpretable results."
      ],
      "metadata": {
        "id": "mC6e_gVJswrZ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LhNfzWxlsY4C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q10. What are some alternatives to Logistic Regression for classification tasks ?"
      ],
      "metadata": {
        "id": "hVC4mvQTs9CF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans. Some alternatives to Logistic Regression for classification tasks include:\n",
        "\n",
        "Decision Trees: Model data using tree-like structures for classification based on feature values.\n",
        "Random Forests: An ensemble of decision trees that improve classification by averaging multiple models.\n",
        "Support Vector Machines (SVM): Finds the optimal hyperplane to separate different classes.\n",
        "K-Nearest Neighbors (KNN): Classifies based on the majority class of the nearest neighbors.\n",
        "Naive Bayes: A probabilistic classifier based on Bayes' theorem, useful for text classification.\n",
        "Neural Networks: Complex models that learn hierarchical patterns in data, effective for large datasets.\n",
        "Each method has its strengths, depending on the complexity and characteristics of the dataset."
      ],
      "metadata": {
        "id": "WJpvq30qs2yq"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KX2-lIg9s5Xa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q11. What are Classification Evaluation Metrics?"
      ],
      "metadata": {
        "id": "WUZHuuKps_4c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans. Classification Evaluation Metrics are used to assess the performance of a classification model. Key metrics include:\n",
        "\n",
        "Accuracy: The proportion of correct predictions (total correct / total predictions).\n",
        "Precision: The proportion of true positives among predicted positives ( ğ‘‡ ğ‘ƒ ğ‘‡ ğ‘ƒ\n",
        "ğ¹ ğ‘ƒ TP+FP TPâ€‹).\n",
        "Recall (Sensitivity): The proportion of true positives among actual positives ( ğ‘‡ ğ‘ƒ ğ‘‡ ğ‘ƒ\n",
        "ğ¹ ğ‘ TP+FN TPâ€‹).\n",
        "F1-Score: The harmonic mean of precision and recall, balancing both ( 2 Ã— ğ‘ƒ ğ‘Ÿ ğ‘’ ğ‘ ğ‘– ğ‘  ğ‘– ğ‘œ ğ‘› Ã— ğ‘… ğ‘’ ğ‘ ğ‘ ğ‘™ ğ‘™ ğ‘ƒ ğ‘Ÿ ğ‘’ ğ‘ ğ‘– ğ‘  ğ‘– ğ‘œ ğ‘›\n",
        "ğ‘… ğ‘’ ğ‘ ğ‘ ğ‘™ ğ‘™ 2Ã— Precision+Recall PrecisionÃ—Recallâ€‹).\n",
        "ROC-AUC: The area under the Receiver Operating Characteristic curve, showing the trade-off between true positive rate and false positive rate.\n",
        "Confusion Matrix: A table showing the counts of true positives (TP), false positives (FP), true negatives (TN), and false negatives (FN).\n",
        "These metrics help evaluate model performance from different perspectives, depending on the task."
      ],
      "metadata": {
        "id": "aPl-dQ2ttDJU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q12. How does class imbalance affect Logistic Regression?"
      ],
      "metadata": {
        "id": "9384HGiRtHSZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans. Class imbalance in Logistic Regression occurs when one class is significantly more frequent than the other. This can lead to:\n",
        "\n",
        "Bias Toward Majority Class: The model may predict the majority class most of the time, reducing accuracy for the minority class.\n",
        "Poor Model Performance: Metrics like accuracy can be misleading, as the model may perform well on the majority class but poorly on the minority class.\n",
        "Underestimating Minority Class: The model may fail to capture important patterns in the minority class, leading to low recall for that class.\n",
        "To address this, techniques like resampling, class weights, or using metrics like precision, recall, and F1-score can help"
      ],
      "metadata": {
        "id": "K5yg9xgZtMVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q13. What is Hyperparameter Tuning in Logistic Regression?"
      ],
      "metadata": {
        "id": "uUTLc2XltULn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans. Hyperparameter tuning in Logistic Regression involves adjusting parameters that control the model's behavior to improve its performance. Key hyperparameters include:\n",
        "\n",
        "Regularization Strength (Î»): Controls the amount of regularization to prevent overfitting. A higher value increases regularization.\n",
        "Solver: Algorithm used to optimize the model (e.g., \"liblinear,\" \"saga,\" \"newton-cg\").\n",
        "Maximum Iterations: Number of iterations the algorithm runs during optimization.\n",
        "Class Weights: Adjusts the weight of each class to handle imbalanced datasets\n",
        "Tuning these hyperparameters helps find the best model configuration for accurate predictions. This can be done through methods like Grid Search or Random Search"
      ],
      "metadata": {
        "id": "Ow-jWXeYtY8k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q14. What are different solvers in Logistic Regression? Which one should be used?"
      ],
      "metadata": {
        "id": "z7voPsQGtcZ7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans. In Logistic Regression, different solvers are used to optimize the cost function:\n",
        "\n",
        "'liblinear': Suitable for small datasets and works well with L1 regularization (Lasso). It uses a coordinate descent algorithm.\n",
        "'newton-cg': Suitable for larger datasets and works well for L2 regularization (Ridge). It uses Newton's method for optimization\n",
        "'lbfgs': An approximation of Newton's method, efficient for large datasets and handles L2 regularization.\n",
        "'saga': A variant of stochastic gradient descent that handles both L1 and L2 regularization, and is efficient for large datasets.\n",
        "Which solver to use?\n",
        "\n",
        "â€¢ For small datasets, 'liblinear' is a good choice. â€¢ For large datasets, 'newton-cg', 'lbfgs', or 'saga' are better. â€¢ Use 'saga' if you need both L1 and L2 regularization."
      ],
      "metadata": {
        "id": "9_My8hz3tfCZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q15. How is Logistic Regression extended for multiclass classification?"
      ],
      "metadata": {
        "id": "YVFMz3ttu8B7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans. Logistic Regression is extended for multiclass classification using two main approaches:\n",
        "\n",
        "One-vs-Rest (OvR): For each class, a separate binary classifier is trained to distinguish that class from all other classes. The final prediction is the class with the highest probability.\n",
        "Softmax Regression (Multinomial Logistic Regression): Generalizes logistic regression to multiple classes by using the softmax function instead of the sigmoid function. It outputs a probability distribution over all classes, and the class with the highest probability is selected.\n",
        "Softmax is typically preferred for multiclass problems as it directly handles all classes in one model."
      ],
      "metadata": {
        "id": "PJirIWSYu_kX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q16. What are the advantages and disadvantages of Logistic Regression?"
      ],
      "metadata": {
        "id": "NDVHKyIkvUI5"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bPiSFXIOvUbg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans. Advantages of Logistic Regression:\n",
        "\n",
        "Simple and Interpretable: Easy to understand and interpret, with clear probabilities for classification.\n",
        "Efficient: Computationally efficient and works well for small to medium-sized datasets.\n",
        "Works well for Linearly Separable Data: Performs well when the classes are linearly separable.\n",
        "Probability Output: Provides probability estimates for class membership.\n",
        "Disadvantages of Logistic Regression:\n",
        "\n",
        "Limited to Linear Boundaries: Struggles with complex relationships or non-linear decision boundaries.\n",
        "Sensitive to Outliers: Outliers can significantly affect the model's performance.\n",
        "Assumes Independence of Features: Assumes that the input features are independent, which may not always be true.\n",
        "Prone to Underfitting: Can underperform when the model is too simple for complex data."
      ],
      "metadata": {
        "id": "LBc9mz1wvWWm"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aeZRKCXUvZHS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q17. What are some use cases of Logistic Regression?"
      ],
      "metadata": {
        "id": "lt692KXIvfUG"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4ukLBa0XvfkZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans. Some common use cases of Logistic Regression include:\n",
        "\n",
        "Spam Detection: Classifying emails as spam or not spam based on features like content and sender.\n",
        "Customer Churn Prediction: Predicting whether a customer will leave a service based on usage patterns.\n",
        "Credit Scoring: Predicting whether a loan applicant will default on a loan based on financial history.\n",
        "Medical Diagnosis: Classifying whether a patient has a certain disease (e.g., cancer or diabetes) based on medical data.\n",
        "Marketing Campaigns: Predicting whether a customer will respond to a promotional offer based on demographics and past behavior.\n",
        "Logistic Regression is widely used in binary classification problems where the outcome is categorical (e.g., yes/no, 0/1)."
      ],
      "metadata": {
        "id": "D4C6IDnqviqL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q18. What is the difference between Softmax Regression and Logistic Regression?"
      ],
      "metadata": {
        "id": "uYLB1k4Evm7M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans. The key difference between Softmax Regression and Logistic Regression lies in the type of classification problem they address:\n",
        "\n",
        "â€¢ Logistic Regression is used for binary classification (two classes), where the output is a probability for one class (0 or 1).\n",
        "\n",
        "â€¢ Softmax Regression (or Multinomial Logistic Regression) is an extension of logistic regression for multiclass classification (more than two classes). It uses the softmax function to compute probabilities for multiple classes, and the class with the highest probability is chosen.\n",
        "\n",
        "In short, Logistic Regression is for binary classification, and Softmax Regression is for multiclass classification."
      ],
      "metadata": {
        "id": "Q8kqgLSpvoXL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q19. How do we choose between One-vs-Rest (OvR) and Softmax for multiclass classification?"
      ],
      "metadata": {
        "id": "AMDnLbvhvsE7"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Nu_sh1g_vwps"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans. Choose between One-vs-Rest (OvR) and Softmax for multiclass classification based on:\n",
        "\n",
        "â€¢ One-vs-Rest (OvR): Best for smaller datasets or when you want to train multiple binary classifiers. It is simpler and can handle more classes, but may not work as well when classes are not well-separated.\n",
        "\n",
        "â€¢ Softmax: Preferred for direct multiclass problems where you want a single model to handle all classes simultaneously. It is better when classes are mutually exclusive and helps avoid the complexity of multiple binary classifiers.\n",
        "\n",
        "Softmax is generally the better choice for pure multiclass problems, while OvR can be useful when flexibility and simplicity are needed."
      ],
      "metadata": {
        "id": "OVQ5LX9ivyo8"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1LVdBlTMvy6D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Q20. How do we interpret coefficients in Logistic Regression ?"
      ],
      "metadata": {
        "id": "bwAsndO7v1vw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ans. In Logistic Regression, the coefficients ( ğ›½ Î²) represent the relationship between each feature and the log-odds of the outcome:\n",
        "\n",
        "â€¢ Interpretation of Coefficients: Each coefficient ( ğ›½ ğ‘— Î² jâ€‹) indicates the change in the log-odds of the target variable for a one-unit increase in the corresponding feature ( ğ‘‹ ğ‘— X jâ€‹), holding other features constant.\n",
        "\n",
        "â€¢ Odds Ratio: The exponentiated coefficient ( ğ‘’ ğ›½ ğ‘— e Î² jâ€‹\n",
        "\n",
        ") gives the odds ratio. It tells you how the odds of the target class change with a one-unit increase in the feature.\n",
        "\n",
        "â€¢ If ğ‘’ ğ›½ ğ‘—\n",
        "\n",
        "1 e Î² jâ€‹\n",
        "\n",
        "1, the odds of the outcome increase as the feature increases.\n",
        "\n",
        "â€¢ If ğ‘’ ğ›½ ğ‘— < 1 e Î² jâ€‹\n",
        "\n",
        "<1, the odds decrease as the feature increases.\n",
        "\n",
        "In summary, the coefficient shows the direction and strength of the relationship between each feature and the target, while the odds ratio provides a more intuitive understanding of the impact."
      ],
      "metadata": {
        "id": "jBTd5zB7v2k6"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f69pj15nv2CM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}